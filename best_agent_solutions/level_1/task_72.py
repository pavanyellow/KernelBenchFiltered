# level 1 index 72 agent name: KernelAgent O3 Mini High speedup: 8.72x

import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple,
                 stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0),
                 output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, stride=stride,
            padding=padding, output_padding=output_padding, groups=groups, bias=bias
        )
        # Pre-convert the convolution weights to float16 and put them into channels_last_3d format.
        # This extra memory formatting tends to allow cuDNN (and Tensor Cores on supported GPUs)
        # to achieve even higher performance.
        self.conv_transpose3d.weight.data = (
            self.conv_transpose3d.weight.data.half().to(memory_format=torch.channels_last_3d)
        )
        if bias and self.conv_transpose3d.bias is not None:
            self.conv_transpose3d.bias.data = self.conv_transpose3d.bias.data.half()

        # Make sure the convolution uses channels_last_3d memory format.
        self.conv_transpose3d = self.conv_transpose3d.to(memory_format=torch.channels_last_3d)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Fuse the input conversion to half precision with the conversion to channels_last_3d.
        # This layout is beneficial for the underlying high‚Äêperformance kernels.
        x = x.to(dtype=torch.half, memory_format=torch.channels_last_3d)
        # The core computation is done in half precision.
        y = self.conv_transpose3d(x)
        # Convert back the output to the original float32 precision.
        return y.to(torch.float32)
